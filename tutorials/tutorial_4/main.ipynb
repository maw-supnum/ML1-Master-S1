{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Torch and Tensorflow on the GPU vs NumPy on the CPU\n",
    "\n",
    "### Exercise 1: Matrix Multiplication\n",
    "\n",
    "In this exercise, we will compare the computation speed of matrix multiplication using `numpy`, `torch`, and `tensorflow`. We will use the `numpy`, `torch`, and `tensorflow` libraries to perform matrix multiplication and compare the computation speed of the three libraries.\n",
    "\n",
    "**Questions**\n",
    "1. import the necessary libraries (numpy, torch, tensorflow)\n",
    "2. create two random matrices of size 1000x1000\n",
    "3. perform matrix multiplication using numpy, torch, and tensorflow and compare the computation speed of the three libraries using timeit function\n",
    "4. conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy \n",
    "\n",
    "### Exercise 2: \n",
    "\n",
    "In this exercise, we will compute the entropy of a binary sequence and compress the sequence using different models. We will compare the entropy of the original and compressed sequences to understand the effectiveness of the compression models.\n",
    "\n",
    "**Questions**\n",
    "1. Create a sequence of 10000 binaries with a specific bernoulli distribution (p=0.99)\n",
    "2. compute the entropy of this distribution using Shannon's entropy formula\n",
    "3. define some models to compress the sequence (encode the sequences of 1)\n",
    "4. pick the best model and compress the sequence (don't forget to include model complexity)\n",
    "4. compute the empirical entropy of the compressed sequence\n",
    "5. compare the entropy of the original and compressed sequences \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Cross-Entropy Convergence\n",
    "\n",
    "### Exercise 3:\n",
    "\n",
    "In this exercise, we will test empirically the convergence of the validation cross-entropy loss to the true cross-entropy loss when the number of samples is large. \n",
    "\n",
    "Consider the following model: \\\n",
    "for each point i represented by $x_i \\in R^2$, we define $y_i = min(|x_i - \\alpha_1|_{\\infty}, |x_i - \\alpha_2|_{\\infty}) - 1$ with $\\alpha_1 = (0, 2)$ and $\\alpha_2 = (0, -2)$ and  $\\epsilon_i \\sim \\mathcal{Ber}(0.3)$. \\\n",
    "We then define two regions $R_1 = \\{x \\in R^2, y \\leq 0\\}$ and $R_2 = \\{x \\in R^2, y > 0\\}$.\\\n",
    "And a border zone $B = \\{x \\in R^2, y = 0\\}$.\\\n",
    "Assume we have two classes for the points $x_i$: $c_0$ and $c_1$. \n",
    "- if $x_i \\in R_1$, $d(x_i,B) \\leq 0.2$ and $\\epsilon_i = 0$, then $c_i = c_0$\n",
    "- if $x_i \\in R_1$, $d(x_i,B) \\leq 0.2$ and $\\epsilon_i = 1$, then $c_i = c_1$\n",
    "- if $x_i \\in R_1$, $d(x_i,B)>0.2$, then $c_i = c_0$\n",
    "- if $x_i \\in R_2$, $d(x_i,B) \\leq 0.2$ and $\\epsilon_i = 0$, then $c_i = c_1$\n",
    "- if $x_i \\in R_2$, $d(x_i,B) \\leq 0.2$ and $\\epsilon_i = 1$, then $c_i = c_0$\n",
    "- if $x_i \\in R_2$, $d(x_i,B)>0.2$, then $c_i = c_1$\n",
    "\n",
    "where $d(x,B) = min_{b \\in B} |x-b|_{\\infty}$.\n",
    "\n",
    "To be able to compute the true cross-entropy loss, we assume that $x_i \\sim \\mathcal{U}([-3.5,3.5]^2)$.\n",
    "\n",
    "\n",
    "**Part 1 - Questions: data vizualisation and loss approximation**\n",
    "1. Generate 1000 points $x_i$ and compute the corresponding $y_i$.\n",
    "2. Plot the points $x_i$ in the plane and color them according to their class.\n",
    "3. compute the theoretical cross-entropy loss of the model.\n",
    "4. compute the empirical cross-entropy loss of the model using the generated points and compare it to the theoretical one.\n",
    "\n",
    "Next, we approximate the true cross-entropy loss using the grid (100x100) of points $x_i$ in $[-3.5,3.5]^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 - Questions: cross-entropy loss of a random forest**\n",
    "1. split the data into a training and a validation set and train a random forest model on the training set.\n",
    "2. On the grid of points, compute the predicted probabilities and the approximation of true cross-entropy loss.\n",
    "3. compare the cross-entropy loss of the model on the validation set to the approximation of the true cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
