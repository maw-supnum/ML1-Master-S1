{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent, Rétropropagation et Réseaux de Neurones Feedforward\n",
    "\n",
    "Ce notebook explore les concepts fondamentaux des réseaux de neurones feedforward, l'algorithme de rétropropagation du gradient, et l'optimisation par descente de gradient stochastique (SGD).\n",
    "\n",
    "## Sommaire\n",
    "1. Introduction aux réseaux de neurones feedforward\n",
    "2. Propagation avant (Forward Propagation)\n",
    "3. Rétropropagation du gradient (Backpropagation)\n",
    "4. Descente de gradient stochastique (SGD)\n",
    "5. Implémentation d'un réseau de neurones simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction aux réseaux de neurones feedforward\n",
    "\n",
    "Les réseaux de neurones feedforward sont les architectures les plus simples de réseaux de neurones artificiels. L'information se propage uniquement dans un sens : de l'entrée vers la sortie, sans boucles de rétroaction.\n",
    "\n",
    "Un réseau de neurones feedforward est généralement organisé en couches :\n",
    "- Une couche d'entrée\n",
    "- Une ou plusieurs couches cachées\n",
    "- Une couche de sortie\n",
    "\n",
    "Chaque couche est composée de neurones (ou unités) qui reçoivent des entrées, effectuent une transformation non-linéaire, et transmettent le résultat à la couche suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Configuration pour la reproductibilité\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Propagation avant (Forward Propagation)\n",
    "\n",
    "La propagation avant est le processus par lequel l'information circule de l'entrée vers la sortie du réseau. Pour chaque neurone, nous calculons la somme pondérée des entrées, puis appliquons une fonction d'activation.\n",
    "\n",
    "Mathématiquement, pour une couche $l$ avec $n^{[l]}$ neurones, la propagation avant est définie par :\n",
    "\n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}$$\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "où :\n",
    "- $Z^{[l]}$ est l'entrée linéaire de la couche $l$\n",
    "- $W^{[l]}$ est la matrice des poids de la couche $l$\n",
    "- $A^{[l-1]}$ est l'activation de la couche précédente\n",
    "- $b^{[l]}$ est le vecteur de biais de la couche $l$\n",
    "- $g^{[l]}$ est la fonction d'activation de la couche $l$\n",
    "- $A^{[l]}$ est l'activation de la couche $l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation des fonctions d'activation courantes\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.power(np.tanh(z), 2)\n",
    "\n",
    "# Visualisation des fonctions d'activation\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(z, sigmoid(z), 'b', label='Sigmoid')\n",
    "plt.plot(z, sigmoid_derivative(z), 'r--', label='Dérivée')\n",
    "plt.legend()\n",
    "plt.title('Sigmoid')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(z, relu(z), 'b', label='ReLU')\n",
    "plt.plot(z, relu_derivative(z), 'r--', label='Dérivée')\n",
    "plt.legend()\n",
    "plt.title('ReLU')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(z, tanh(z), 'b', label='Tanh')\n",
    "plt.plot(z, tanh_derivative(z), 'r--', label='Dérivée')\n",
    "plt.legend()\n",
    "plt.title('Tanh')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rétropropagation du gradient (Backpropagation)\n",
    "\n",
    "La rétropropagation est l'algorithme utilisé pour calculer les gradients de la fonction de perte par rapport aux paramètres du réseau (poids et biais). Ces gradients sont ensuite utilisés pour mettre à jour les paramètres lors de l'optimisation.\n",
    "\n",
    "Le principe de la rétropropagation est basé sur la règle de dérivation en chaîne. Pour une couche $l$, nous calculons :\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[l]}} \\cdot \\frac{\\partial A^{[l]}}{\\partial Z^{[l]}}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} \\cdot \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} \\cdot A^{[l-1]T}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} \\cdot \\frac{\\partial Z^{[l]}}{\\partial b^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = W^{[l]T} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}$$\n",
    "\n",
    "où $\\mathcal{L}$ est la fonction de perte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Descente de gradient stochastique (SGD)\n",
    "\n",
    "La descente de gradient stochastique est un algorithme d'optimisation utilisé pour minimiser la fonction de perte en ajustant les paramètres du réseau dans la direction opposée au gradient.\n",
    "\n",
    "L'équation de mise à jour des paramètres est :\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta \\mathcal{L}(\\theta)$$\n",
    "\n",
    "où :\n",
    "- $\\theta$ représente un paramètre du réseau (poids ou biais)\n",
    "- $\\alpha$ est le taux d'apprentissage\n",
    "- $\\nabla_\\theta \\mathcal{L}(\\theta)$ est le gradient de la fonction de perte par rapport au paramètre $\\theta$\n",
    "\n",
    "La spécificité de la descente de gradient stochastique est qu'elle utilise un sous-ensemble aléatoire de données (mini-batch) à chaque itération, ce qui rend l'optimisation plus efficace et permet d'échapper à certains minima locaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implémentation d'un réseau de neurones simple\n",
    "\n",
    "Nous allons maintenant implémenter un réseau de neurones feedforward simple avec NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération d'un jeu de données synthétique\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Conversion des classes pour correspondre à notre fonction d'activation de sortie (sigmoid)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_dims, activations):\n",
    "        \"\"\"\n",
    "        Initialise un réseau de neurones feedforward.\n",
    "        \n",
    "        Arguments:\n",
    "        layer_dims -- liste contenant les dimensions de chaque couche\n",
    "        activations -- liste contenant les fonctions d'activation de chaque couche\n",
    "        \"\"\"\n",
    "        self.L = len(layer_dims) - 1  # nombre de couches\n",
    "        self.layer_dims = layer_dims\n",
    "        self.activations = activations\n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Initialisation des poids et biais\n",
    "        for l in range(1, self.L + 1):\n",
    "            # He initialization pour ReLU, Xavier pour sigmoid/tanh\n",
    "            if activations[l-1] == relu:\n",
    "                self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "            else:\n",
    "                self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1 / layer_dims[l-1])\n",
    "                \n",
    "            self.parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Effectue la propagation avant à travers le réseau.\n",
    "        \n",
    "        Arguments:\n",
    "        X -- données d'entrée, de forme (nb_examples, nb_features)\n",
    "        \n",
    "        Returns:\n",
    "        AL -- activation de la dernière couche\n",
    "        \"\"\"\n",
    "        A = X.T  # Transposée pour avoir la forme (nb_features, nb_examples)\n",
    "        self.cache[\"A0\"] = A\n",
    "        \n",
    "        for l in range(1, self.L + 1):\n",
    "            Z = np.dot(self.parameters[f\"W{l}\"], A) + self.parameters[f\"b{l}\"]\n",
    "            self.cache[f\"Z{l}\"] = Z\n",
    "            \n",
    "            A = self.activations[l-1](Z)\n",
    "            self.cache[f\"A{l}\"] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Calcule la fonction de coût (entropie croisée binaire).\n",
    "        \n",
    "        Arguments:\n",
    "        AL -- activation de la dernière couche, de forme (1, nb_examples)\n",
    "        Y -- vecteur des étiquettes, de forme (1, nb_examples)\n",
    "        \n",
    "        Returns:\n",
    "        cost -- coût\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Entropie croisée binaire\n",
    "        cost = -1/m * np.sum(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
    "        \n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def backward_propagation(self, Y):\n",
    "        \"\"\"\n",
    "        Effectue la rétropropagation pour calculer les gradients.\n",
    "        \n",
    "        Arguments:\n",
    "        Y -- vecteur des étiquettes, de forme (1, nb_examples)\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        Y = Y.T  # Transposée pour avoir la forme (1, nb_examples)\n",
    "        \n",
    "        # Initialisation de dA pour la dernière couche (dérivée de l'entropie croisée binaire)\n",
    "        dA = - (np.divide(Y, self.cache[f\"A{self.L}\"] + 1e-8) - np.divide(1 - Y, 1 - self.cache[f\"A{self.L}\"] + 1e-8))\n",
    "        \n",
    "        for l in reversed(range(1, self.L + 1)):\n",
    "            Z = self.cache[f\"Z{l}\"]\n",
    "            \n",
    "            # Dérivée de la fonction d'activation\n",
    "            if self.activations[l-1] == sigmoid:\n",
    "                dZ = dA * sigmoid_derivative(Z)\n",
    "            elif self.activations[l-1] == relu:\n",
    "                dZ = dA * relu_derivative(Z)\n",
    "            elif self.activations[l-1] == tanh:\n",
    "                dZ = dA * tanh_derivative(Z)\n",
    "            \n",
    "            A_prev = self.cache[f\"A{l-1}\"]\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            self.gradients[f\"dW{l}\"] = 1/m * np.dot(dZ, A_prev.T)\n",
    "            self.gradients[f\"db{l}\"] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 1:\n",
    "                dA = np.dot(self.parameters[f\"W{l}\"].T, dZ)\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Met à jour les paramètres en utilisant la descente de gradient.\n",
    "        \n",
    "        Arguments:\n",
    "        learning_rate -- taux d'apprentissage\n",
    "        \"\"\"\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters[f\"W{l}\"] -= learning_rate * self.gradients[f\"dW{l}\"]\n",
    "            self.parameters[f\"b{l}\"] -= learning_rate * self.gradients[f\"db{l}\"]\n",
    "    \n",
    "    def train(self, X, Y, num_iterations, learning_rate, batch_size=32, print_cost=False):\n",
    "        \"\"\"\n",
    "        Entraîne le réseau de neurones en utilisant la descente de gradient stochastique.\n",
    "        \n",
    "        Arguments:\n",
    "        X -- données d'entrée, de forme (nb_examples, nb_features)\n",
    "        Y -- vecteur des étiquettes, de forme (nb_examples, 1)\n",
    "        num_iterations -- nombre d'itérations\n",
    "        learning_rate -- taux d'apprentissage\n",
    "        batch_size -- taille des mini-batchs\n",
    "        print_cost -- si True, affiche le coût toutes les 100 itérations\n",
    "        \"\"\"\n",
    "        costs = []\n",
    "        \n",
    "        # Conversion des données pour assurer la bonne forme\n",
    "        Y = Y.reshape(-1, 1)\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            # Choix aléatoire d'un mini-batch\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            for k in range(0, X.shape[0], batch_size):\n",
    "                batch_indices = indices[k:k+batch_size]\n",
    "                X_batch = X[batch_indices]\n",
    "                Y_batch = Y[batch_indices]\n",
    "                \n",
    "                # Propagation avant\n",
    "                AL = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # Calcul du coût\n",
    "                cost = self.compute_cost(AL, Y_batch.T)\n",
    "                \n",
    "                # Rétropropagation\n",
    "                self.backward_propagation(Y_batch.T)\n",
    "                \n",
    "                # Mise à jour des paramètres\n",
    "                self.update_parameters(learning_rate)\n",
    "            \n",
    "            # Affichage du coût\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print(f\"Coût après itération {i}: {cost}\")\n",
    "                costs.append(cost)\n",
    "        \n",
    "        return costs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Prédit les classes pour les données X.\n",
    "        \n",
    "        Arguments:\n",
    "        X -- données d'entrée, de forme (nb_examples, nb_features)\n",
    "        \n",
    "        Returns:\n",
    "        predictions -- vecteur des prédictions, de forme (nb_examples, 1)\n",
    "        \"\"\"\n",
    "        AL = self.forward_propagation(X)\n",
    "        predictions = (AL > 0.5).astype(int)\n",
    "        \n",
    "        return predictions.T  # Transposée pour avoir la forme (nb_examples, 1)\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        \"\"\"\n",
    "        Évalue la précision du modèle sur les données X avec les étiquettes Y.\n",
    "        \n",
    "        Arguments:\n",
    "        X -- données d'entrée, de forme (nb_examples, nb_features)\n",
    "        Y -- vecteur des étiquettes, de forme (nb_examples, 1)\n",
    "        \n",
    "        Returns:\n",
    "        accuracy -- précision du modèle\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'architecture du réseau\n",
    "layer_dims = [X_train.shape[1], 64, 32, 1]  # [entrée, couche cachée 1, couche cachée 2, sortie]\n",
    "activations = [relu, relu, sigmoid]  # Fonctions d'activation pour chaque couche\n",
    "\n",
    "# Création et entraînement du réseau avec NumPy\n",
    "nn = NeuralNetwork(layer_dims, activations)\n",
    "\n",
    "start_time = time.time()\n",
    "costs = nn.train(X_train, y_train, num_iterations=500, learning_rate=0.01, batch_size=32, print_cost=True)\n",
    "numpy_time = time.time() - start_time\n",
    "\n",
    "# Évaluation du modèle\n",
    "train_accuracy = nn.evaluate(X_train, y_train)\n",
    "test_accuracy = nn.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nPrécision sur l'ensemble d'entraînement: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Précision sur l'ensemble de test: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Temps d'entraînement avec NumPy: {numpy_time:.2f} secondes\")\n",
    "\n",
    "# Visualisation de la courbe d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, 500, 100), costs)\n",
    "plt.xlabel('Itérations')\n",
    "plt.ylabel('Coût')\n",
    "plt.title('Évolution du coût pendant l\\'entraînement')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
