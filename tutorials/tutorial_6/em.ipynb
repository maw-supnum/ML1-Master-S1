{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel sur l'Algorithme EM et l'Apprentissage par Renforcement\n",
    "\n",
    "Ce notebook présente deux concepts fondamentaux en apprentissage automatique :\n",
    "1. L'algorithme d'Expectation-Maximization (EM)\n",
    "2. L'apprentissage par renforcement (Reinforcement Learning)\n",
    "\n",
    "Nous utiliserons principalement NumPy pour implémenter ces algorithmes et générer des données pour nos expériences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1: Algorithme d'Expectation-Maximization (EM)\n",
    "\n",
    "L'algorithme EM est une méthode itérative pour trouver les estimations du maximum de vraisemblance des paramètres dans des modèles statistiques, où le modèle dépend de variables latentes non observées.\n",
    "\n",
    "### Application: Mélange de Gaussiennes (Gaussian Mixture Model)\n",
    "\n",
    "L'un des cas d'utilisation les plus courants de l'algorithme EM est l'estimation des paramètres d'un mélange de distributions gaussiennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Configurer matplotlib pour des graphiques plus agréables\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération de données pour le mélange de gaussiennes\n",
    "\n",
    "Commençons par générer des données qui suivent un mélange de 3 distributions gaussiennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour la génération des données\n",
    "n_samples = 1000\n",
    "n_components = 3\n",
    "\n",
    "# Les vraies moyennes de nos gaussiennes\n",
    "true_means = np.array([[-2, -2], [0, 0], [2, 2]])\n",
    "# Les vraies covariances\n",
    "true_covs = np.array([\n",
    "    [[0.5, 0], [0, 0.5]],\n",
    "    [[1, 0], [0, 1]],\n",
    "    [[0.5, 0], [0, 0.5]]\n",
    "])\n",
    "# Les vrais poids des composantes\n",
    "true_weights = np.array([0.3, 0.4, 0.3])\n",
    "\n",
    "# Générer les données\n",
    "X, y = make_blobs(n_samples=n_samples, centers=true_means, \n",
    "                  cluster_std=np.sqrt(np.array([0.5, 1, 0.5])),\n",
    "                  random_state=42)\n",
    "\n",
    "# Appliquer les poids (rééchantillonner pour respecter les proportions)\n",
    "indices = np.random.choice(np.arange(n_samples), size=n_samples, \n",
    "                         p=np.bincount(y) / n_samples)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Visualiser les données générées\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis', alpha=0.7)\n",
    "plt.title('Données générées: Mélange de 3 gaussiennes')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de l'algorithme EM pour le mélange de gaussiennes\n",
    "\n",
    "L'algorithme EM consiste en deux étapes principales qui sont répétées jusqu'à convergence :\n",
    "1. **Étape E (Expectation)** : Calculer les probabilités postérieures (responsabilités) de chaque composante pour chaque point de données\n",
    "2. **Étape M (Maximization)** : Mettre à jour les paramètres (moyennes, covariances, poids) en utilisant ces responsabilités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureEM:\n",
    "    def __init__(self, n_components=3, max_iter=100, tol=1e-6, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def initialize_parameters(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialiser les moyennes en sélectionnant aléatoirement des points de données\n",
    "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means = X[indices]\n",
    "        \n",
    "        # Initialiser les covariances à l'identité\n",
    "        self.covariances = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "        \n",
    "        # Initialiser les poids uniformément\n",
    "        self.weights = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "    def e_step(self, X):\n",
    "        \"\"\"Étape E: Calcul des responsabilités\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        # Calculer la densité de probabilité pour chaque point et chaque composante\n",
    "        for k in range(self.n_components):\n",
    "            responsibilities[:, k] = self.weights[k] * multivariate_normal.pdf(\n",
    "                X, mean=self.means[k], cov=self.covariances[k])\n",
    "        \n",
    "        # Normaliser pour obtenir les probabilités postérieures\n",
    "        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n",
    "        responsibilities /= responsibilities_sum\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def m_step(self, X, responsibilities):\n",
    "        \"\"\"Étape M: Mise à jour des paramètres\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Calculer les responsabilités totales pour chaque composante\n",
    "        resp_sum = responsibilities.sum(axis=0)\n",
    "        \n",
    "        # Mettre à jour les poids\n",
    "        self.weights = resp_sum / n_samples\n",
    "        \n",
    "        # Mettre à jour les moyennes\n",
    "        self.means = np.dot(responsibilities.T, X) / resp_sum[:, np.newaxis]\n",
    "        \n",
    "        # Mettre à jour les covariances\n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means[k]\n",
    "            self.covariances[k] = np.dot(responsibilities[:, k] * diff.T, diff) / resp_sum[k]\n",
    "            \n",
    "            # Ajouter une petite régularisation pour éviter les problèmes numériques\n",
    "            self.covariances[k] += 1e-6 * np.eye(X.shape[1])\n",
    "    \n",
    "    def compute_log_likelihood(self, X):\n",
    "        \"\"\"Calculer la log-vraisemblance des données\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        likelihood = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            likelihood[:, k] = self.weights[k] * multivariate_normal.pdf(\n",
    "                X, mean=self.means[k], cov=self.covariances[k])\n",
    "        \n",
    "        return np.sum(np.log(np.sum(likelihood, axis=1)))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Ajuster le modèle aux données\"\"\"\n",
    "        self.initialize_parameters(X)\n",
    "        \n",
    "        log_likelihood_history = []\n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Étape E\n",
    "            responsibilities = self.e_step(X)\n",
    "            \n",
    "            # Étape M\n",
    "            self.m_step(X, responsibilities)\n",
    "            \n",
    "            # Calculer la log-vraisemblance\n",
    "            log_likelihood = self.compute_log_likelihood(X)\n",
    "            log_likelihood_history.append(log_likelihood)\n",
    "            \n",
    "            # Vérifier la convergence\n",
    "            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "                \n",
    "            prev_log_likelihood = log_likelihood\n",
    "            \n",
    "            # Afficher les progrès tous les 5 itérations\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Itération {i+1}/{self.max_iter}, Log-vraisemblance: {log_likelihood:.2f}\")\n",
    "        \n",
    "        self.log_likelihood_history = log_likelihood_history\n",
    "        self.n_iter_ = i + 1\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Prédire les clusters pour les données X\"\"\"\n",
    "        responsibilities = self.e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustement du modèle et visualisation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mélanger les données (pour oublier les vrais clusters)\n",
    "np.random.shuffle(X)\n",
    "\n",
    "# Ajuster le modèle EM\n",
    "gmm = GaussianMixtureEM(n_components=3, max_iter=50, random_state=42)\n",
    "gmm.fit(X)\n",
    "\n",
    "# Prédire les clusters\n",
    "y_pred = gmm.predict(X)\n",
    "\n",
    "# Afficher la convergence de la log-vraisemblance\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(gmm.log_likelihood_history)\n",
    "plt.xlabel('Itérations')\n",
    "plt.ylabel('Log-vraisemblance')\n",
    "plt.title('Convergence de la log-vraisemblance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les résultats\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Données avec prédictions\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=30, cmap='viridis', alpha=0.7)\n",
    "plt.title('Clusters détectés par EM')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "\n",
    "# Visualiser les gaussiennes estimées\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=30, cmap='viridis', alpha=0.2)\n",
    "\n",
    "# Créer une grille pour visualiser les contours de densité\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Calculer la densité pour chaque point de la grille\n",
    "Z = np.zeros((grid.shape[0], gmm.n_components))\n",
    "for k in range(gmm.n_components):\n",
    "    Z[:, k] = gmm.weights[k] * multivariate_normal.pdf(\n",
    "        grid, mean=gmm.means[k], cov=gmm.covariances[k])\n",
    "Z = Z.sum(axis=1).reshape(xx.shape)\n",
    "\n",
    "# Tracer les contours de densité\n",
    "plt.contour(xx, yy, Z, levels=5, cmap='viridis')\n",
    "\n",
    "# Tracer les centres des gaussiennes\n",
    "plt.scatter(gmm.means[:, 0], gmm.means[:, 1], c='red', s=200, marker='x')\n",
    "\n",
    "plt.title('Gaussiennes estimées')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparer avec les vrais paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Comparaison des paramètres estimés vs réels ===\")\n",
    "print(\"\\nMoyennes estimées:\")\n",
    "print(gmm.means)\n",
    "print(\"\\nVraies moyennes:\")\n",
    "print(true_means)\n",
    "\n",
    "print(\"\\nCovariances estimées:\")\n",
    "for i, cov in enumerate(gmm.covariances):\n",
    "    print(f\"Composante {i+1}:\\n{cov}\")\n",
    "print(\"\\nVraies covariances:\")\n",
    "for i, cov in enumerate(true_covs):\n",
    "    print(f\"Composante {i+1}:\\n{cov}\")\n",
    "\n",
    "print(\"\\nPoids estimés:\")\n",
    "print(gmm.weights)\n",
    "print(\"\\nVrais poids:\")\n",
    "print(true_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple avancé: EM pour un mélange bimodal en 1D\n",
    "\n",
    "Illustrons encore le fonctionnement de l'algorithme EM pour un mélange de deux gaussiennes en une dimension. Cela nous permettra de mieux visualiser les étapes de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des données 1D suivant un mélange de deux gaussiennes\n",
    "np.random.seed(42)\n",
    "n1, n2 = 300, 700  # Nombre d'échantillons par gaussienne\n",
    "mu1, sigma1 = -2, 0.5  # Moyenne et écart-type de la première gaussienne\n",
    "mu2, sigma2 = 2, 1.0   # Moyenne et écart-type de la deuxième gaussienne\n",
    "\n",
    "# Générer les échantillons\n",
    "samples1 = np.random.normal(mu1, sigma1, n1)\n",
    "samples2 = np.random.normal(mu2, sigma2, n2)\n",
    "samples = np.concatenate([samples1, samples2])\n",
    "np.random.shuffle(samples)  # Mélanger les échantillons\n",
    "\n",
    "# Convertir en array 2D pour notre algorithme\n",
    "X_1d = samples.reshape(-1, 1)\n",
    "\n",
    "# Visualiser l'histogramme des données\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(samples, bins=30, density=True, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution des données: mélange de deux gaussiennes')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Densité')\n",
    "\n",
    "# Tracer les vraies densités\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "true_density1 = n1 / (n1 + n2) * (1 / (sigma1 * np.sqrt(2 * np.pi))) * np.exp(-(x - mu1)**2 / (2 * sigma1**2))\n",
    "true_density2 = n2 / (n1 + n2) * (1 / (sigma2 * np.sqrt(2 * np.pi))) * np.exp(-(x - mu2)**2 / (2 * sigma2**2))\n",
    "plt.plot(x, true_density1, 'r-', lw=2, label=f'Gaussienne 1: μ={mu1}, σ={sigma1}')\n",
    "plt.plot(x, true_density2, 'g-', lw=2, label=f'Gaussienne 2: μ={mu2}, σ={sigma2}')\n",
    "plt.plot(x, true_density1 + true_density2, 'k--', lw=2, label='Mélange total')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster notre modèle EM au mélange 1D\n",
    "gmm_1d = GaussianMixtureEM(n_components=2, max_iter=30, random_state=42)\n",
    "gmm_1d.fit(X_1d)\n",
    "\n",
    "# Récupérer les paramètres estimés\n",
    "estimated_weights = gmm_1d.weights\n",
    "estimated_means = gmm_1d.means.flatten()\n",
    "estimated_stds = np.sqrt(gmm_1d.covariances.flatten())\n",
    "estimated_cluster = gmm_1d.predict(X_1d)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"=== Paramètres estimés ===\")\n",
    "print(f\"Poids: {estimated_weights}\")\n",
    "print(f\"Moyennes: {estimated_means}\")\n",
    "print(f\"Écarts-types: {estimated_stds}\")\n",
    "\n",
    "print(\"\\n=== Vrais paramètres ===\")\n",
    "print(f\"Poids: [{n1/(n1+n2):.3f}, {n2/(n1+n2):.3f}]\")\n",
    "print(f\"Moyennes: [{mu1}, {mu2}]\")\n",
    "print(f\"Écarts-types: [{sigma1}, {sigma2}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les résultats\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(samples, bins=30, density=True, alpha=0.5, color='skyblue')\n",
    "\n",
    "# Tracer les densités estimées\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "est_density1 = estimated_weights[0] * (1 / (estimated_stds[0] * np.sqrt(2 * np.pi))) * np.exp(-(x - estimated_means[0])**2 / (2 * estimated_stds[0]**2))\n",
    "est_density2 = estimated_weights[1] * (1 / (estimated_stds[1] * np.sqrt(2 * np.pi))) * np.exp(-(x - estimated_means[1])**2 / (2 * estimated_stds[1]**2))\n",
    "\n",
    "plt.plot(x, est_density1, 'r-', lw=2, \n",
    "         label=f'Gaussienne estimée 1: π={estimated_weights[0]:.2f}, μ={estimated_means[0]:.2f}, σ={estimated_stds[0]:.2f}')\n",
    "plt.plot(x, est_density2, 'g-', lw=2, \n",
    "         label=f'Gaussienne estimée 2: π={estimated_weights[1]:.2f}, μ={estimated_means[1]:.2f}, σ={estimated_stds[1]:.2f}')\n",
    "plt.plot(x, est_density1 + est_density2, 'k--', lw=2, label='Mélange estimé total')\n",
    "\n",
    "plt.title('Résultats de l\\'algorithme EM: Mélange de gaussiennes en 1D')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Densité')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
