{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction au Reinforcement Learning\n",
    "\n",
    "## Le Dilemme Exploration/Exploitation\n",
    "\n",
    "Dans ce tutoriel, nous allons explorer l'un des concepts fondamentaux du reinforcement learning : le dilemme exploration/exploitation. \n",
    "\n",
    "Le **dilemme exploration/exploitation** fait référence au compromis entre :\n",
    "- **Explorer** de nouvelles actions pour collecter plus d'informations sur l'environnement\n",
    "- **Exploiter** les connaissances actuelles pour maximiser la récompense immédiate\n",
    "\n",
    "Nous allons illustrer ce concept à travers le problème classique du **bandit manchot à plusieurs bras** (multi-armed bandit), qui constitue une introduction parfaite au reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configuration pour des graphiques plus jolis\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)  # Pour la reproductibilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Le problème du bandit manchot à plusieurs bras\n",
    "\n",
    "Imaginons que nous sommes face à plusieurs machines à sous (bandits manchots). Chaque machine a une probabilité inconnue de donner une récompense. Notre objectif est de maximiser notre gain total sur un nombre fini d'essais.\n",
    "\n",
    "### Modélisation du problème\n",
    "\n",
    "- Nous avons `k` machines à sous (bras)\n",
    "- Chaque machine `i` a une distribution de probabilité inconnue avec une espérance de récompense μ_i\n",
    "- À chaque tour, nous choisissons une machine et recevons une récompense\n",
    "- Notre objectif est de maximiser la somme des récompenses sur `T` tours\n",
    "\n",
    "Commençons par créer une classe pour simuler les machines à sous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditEnvironment:\n",
    "    def __init__(self, k=10, distribution_type='bernoulli'):\n",
    "        \"\"\"Initialise un environnement de bandit à k bras\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : int\n",
    "            Nombre de bras (machines)\n",
    "        distribution_type : str\n",
    "            Type de distribution ('bernoulli' ou 'gaussian')\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distribution_type = distribution_type\n",
    "        \n",
    "        # Génération des vraies moyennes pour chaque bras\n",
    "        if distribution_type == 'bernoulli':\n",
    "            # Pour les distributions de Bernoulli, moyennes entre 0 et 1\n",
    "            self.true_means = np.random.random(k)\n",
    "        else:  # 'gaussian'\n",
    "            # Pour les distributions gaussiennes, moyennes entre 0 et 10\n",
    "            self.true_means = np.random.normal(5, 2, k)\n",
    "        \n",
    "        # Stockage du meilleur bras et de sa moyenne\n",
    "        self.optimal_arm = np.argmax(self.true_means)\n",
    "        self.optimal_mean = self.true_means[self.optimal_arm]\n",
    "    \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Tire le bras spécifié et retourne la récompense\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        arm : int\n",
    "            Indice du bras à tirer\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Récompense obtenue\n",
    "        \"\"\"\n",
    "        if arm < 0 or arm >= self.k:\n",
    "            raise ValueError(f\"Bras invalide. Doit être entre 0 et {self.k-1}\")\n",
    "            \n",
    "        # Génération de la récompense selon la distribution\n",
    "        if self.distribution_type == 'bernoulli':\n",
    "            # Pour Bernoulli, 1 avec probabilité true_means[arm], 0 sinon\n",
    "            return np.random.binomial(1, self.true_means[arm])\n",
    "        else:  # 'gaussian'\n",
    "            # Pour Gaussian, centrée sur true_means[arm] avec écart-type 1\n",
    "            return np.random.normal(self.true_means[arm], 1)\n",
    "    \n",
    "    def visualize_arms(self):\n",
    "        \"\"\"Visualise les distributions de récompense des bras\"\"\"\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        # Barplot des vraies moyennes\n",
    "        plt.bar(range(self.k), self.true_means, alpha=0.7)\n",
    "        plt.axhline(y=self.optimal_mean, color='r', linestyle='--', \n",
    "                   label=f'Bras optimal (μ={self.optimal_mean:.3f})')\n",
    "        \n",
    "        plt.xlabel('Bras')\n",
    "        plt.ylabel('Moyenne de récompense (μ)')\n",
    "        plt.title('Distribution des récompenses moyennes par bras')\n",
    "        plt.xticks(range(self.k))\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un environnement à 10 bras et visualisons les moyennes de récompense des différents bras :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un environnement à 10 bras avec distributions de Bernoulli\n",
    "env = BanditEnvironment(k=10, distribution_type='bernoulli')\n",
    "env.visualize_arms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stratégies pour résoudre le dilemme exploration/exploitation\n",
    "\n",
    "Nous allons maintenant implémenter différentes stratégies pour résoudre le problème du bandit manchot, chacune avec une approche différente du dilemme exploration/exploitation :\n",
    "\n",
    "1. **Stratégie ε-greedy** : Choisit le meilleur bras connu avec probabilité 1-ε, et explore aléatoirement avec probabilité ε\n",
    "2. **Stratégie UCB (Upper Confidence Bound)** : Utilise une borne supérieure de confiance pour équilibrer exploration et exploitation\n",
    "3. **Stratégie Thompson Sampling** : Utilise une approche bayésienne pour modéliser l'incertitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditAlgorithm:\n",
    "    def __init__(self, k, algorithm_type='epsilon-greedy', **kwargs):\n",
    "        \"\"\"Initialise un algorithme pour résoudre le problème du bandit\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : int\n",
    "            Nombre de bras\n",
    "        algorithm_type : str\n",
    "            Type d'algorithme ('epsilon-greedy', 'ucb', 'thompson')\n",
    "        **kwargs : dict\n",
    "            Paramètres spécifiques à l'algorithme\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.algorithm_type = algorithm_type\n",
    "        \n",
    "        # Initialisation des estimations de moyenne et du nombre de tirages\n",
    "        self.q_values = np.zeros(k)  # Estimations des moyennes\n",
    "        self.n_pulls = np.zeros(k)   # Nombre de tirages par bras\n",
    "        self.t = 0                   # Nombre total de tirages\n",
    "        \n",
    "        # Paramètres spécifiques aux algorithmes\n",
    "        if algorithm_type == 'epsilon-greedy':\n",
    "            self.epsilon = kwargs.get('epsilon', 0.1)\n",
    "        elif algorithm_type == 'ucb':\n",
    "            self.c = kwargs.get('c', 2.0)  # Paramètre de confiance\n",
    "        elif algorithm_type == 'thompson':\n",
    "            # Pour Thompson Sampling avec distribution de Bernoulli\n",
    "            self.alpha = np.ones(k)  # Succès + 1 (prior)\n",
    "            self.beta = np.ones(k)   # Échecs + 1 (prior)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        \"\"\"Sélectionne un bras selon la stratégie de l'algorithme\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        int\n",
    "            Indice du bras choisi\n",
    "        \"\"\"\n",
    "        if np.min(self.n_pulls) == 0:\n",
    "            # Si certains bras n'ont jamais été tirés, on en choisit un\n",
    "            return np.where(self.n_pulls == 0)[0][0]\n",
    "        \n",
    "        if self.algorithm_type == 'epsilon-greedy':\n",
    "            # Epsilon-greedy: exploration avec proba epsilon, exploitation sinon\n",
    "            if np.random.random() < self.epsilon:\n",
    "                return np.random.randint(self.k)  # Exploration\n",
    "            else:\n",
    "                return np.argmax(self.q_values)   # Exploitation\n",
    "                \n",
    "        elif self.algorithm_type == 'ucb':\n",
    "            # UCB: choisit le bras avec la borne supérieure de confiance la plus élevée\n",
    "            ucb_values = self.q_values + self.c * np.sqrt(np.log(self.t) / self.n_pulls)\n",
    "            return np.argmax(ucb_values)\n",
    "            \n",
    "        elif self.algorithm_type == 'thompson':\n",
    "            # Thompson Sampling: échantillonne de la distribution a posteriori\n",
    "            theta_samples = np.random.beta(self.alpha, self.beta)\n",
    "            return np.argmax(theta_samples)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Met à jour les estimations après avoir tiré un bras\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        arm : int\n",
    "            Indice du bras tiré\n",
    "        reward : float\n",
    "            Récompense obtenue\n",
    "        \"\"\"\n",
    "        # Mise à jour du nombre de tirages\n",
    "        self.n_pulls[arm] += 1\n",
    "        self.t += 1\n",
    "        \n",
    "        # Mise à jour de l'estimation de la moyenne (moyenne incrémentale)\n",
    "        self.q_values[arm] += (reward - self.q_values[arm]) / self.n_pulls[arm]\n",
    "        \n",
    "        # Mise à jour spécifique pour Thompson Sampling\n",
    "        if self.algorithm_type == 'thompson':\n",
    "            if reward == 1:  # Succès\n",
    "                self.alpha[arm] += 1\n",
    "            else:  # Échec\n",
    "                self.beta[arm] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation et comparaison des stratégies\n",
    "\n",
    "Nous allons maintenant comparer les performances des différentes stratégies sur notre environnement de bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, algorithm, n_steps=1000):\n",
    "    \"\"\"Exécute une expérience avec un algorithme sur un environnement\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : BanditEnvironment\n",
    "        Environnement à utiliser\n",
    "    algorithm : BanditAlgorithm\n",
    "        Algorithme à tester\n",
    "    n_steps : int\n",
    "        Nombre d'étapes de simulation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (rewards, optimal_actions)\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(n_steps)\n",
    "    optimal_actions = np.zeros(n_steps)\n",
    "    \n",
    "    for t in range(n_steps):\n",
    "        # Sélection du bras\n",
    "        arm = algorithm.select_arm()\n",
    "        \n",
    "        # Vérification si l'action est optimale\n",
    "        optimal_actions[t] = 1 if arm == env.optimal_arm else 0\n",
    "        \n",
    "        # Tirage du bras et obtention de la récompense\n",
    "        reward = env.pull(arm)\n",
    "        rewards[t] = reward\n",
    "        \n",
    "        # Mise à jour de l'algorithme\n",
    "        algorithm.update(arm, reward)\n",
    "    \n",
    "    return rewards, optimal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de l'expérience\n",
    "n_steps = 5000\n",
    "n_runs = 100  # Nombre d'exécutions pour moyenner les résultats\n",
    "k = 10        # Nombre de bras\n",
    "\n",
    "# Création des algorithmes à comparer\n",
    "algorithms = {\n",
    "    'ε-greedy (ε=0.1)': {'type': 'epsilon-greedy', 'params': {'epsilon': 0.1}},\n",
    "    'ε-greedy (ε=0.01)': {'type': 'epsilon-greedy', 'params': {'epsilon': 0.01}},\n",
    "    'UCB (c=2)': {'type': 'ucb', 'params': {'c': 2}},\n",
    "    'Thompson Sampling': {'type': 'thompson', 'params': {}}\n",
    "}\n",
    "\n",
    "# Stockage des résultats\n",
    "all_rewards = {name: np.zeros((n_runs, n_steps)) for name in algorithms}\n",
    "all_optimal = {name: np.zeros((n_runs, n_steps)) for name in algorithms}\n",
    "\n",
    "# Exécution des expériences\n",
    "for run in tqdm(range(n_runs), desc=\"Runs\"):\n",
    "    # Création d'un nouvel environnement pour chaque exécution\n",
    "    env = BanditEnvironment(k=k, distribution_type='bernoulli')\n",
    "    \n",
    "    for name, algo_config in algorithms.items():\n",
    "        # Création de l'algorithme\n",
    "        algorithm = BanditAlgorithm(\n",
    "            k=k, \n",
    "            algorithm_type=algo_config['type'], \n",
    "            **algo_config['params']\n",
    "        )\n",
    "        \n",
    "        # Exécution de l'expérience\n",
    "        rewards, optimal_actions = run_experiment(env, algorithm, n_steps)\n",
    "        \n",
    "        # Stockage des résultats\n",
    "        all_rewards[name][run] = rewards\n",
    "        all_optimal[name][run] = optimal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des moyennes et intervalles de confiance\n",
    "avg_rewards = {name: np.mean(rewards, axis=0) for name, rewards in all_rewards.items()}\n",
    "avg_optimal = {name: np.mean(optimal, axis=0) for name, optimal in all_optimal.items()}\n",
    "\n",
    "# Calcul des récompenses cumulatives moyennes\n",
    "cumulative_rewards = {name: np.cumsum(rewards) / np.arange(1, n_steps + 1) \n",
    "                      for name, rewards in avg_rewards.items()}\n",
    "\n",
    "# Visualisation des résultats\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Graphique des récompenses moyennes au fil du temps\n",
    "plt.subplot(2, 1, 1)\n",
    "for name, rewards in cumulative_rewards.items():\n",
    "    plt.plot(rewards, label=name)\n",
    "plt.xlabel('Étapes')\n",
    "plt.ylabel('Récompense moyenne')\n",
    "plt.title('Récompense moyenne au fil du temps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Graphique du pourcentage d'actions optimales au fil du temps\n",
    "plt.subplot(2, 1, 2)\n",
    "window = 200  # Fenêtre glissante pour lisser la courbe\n",
    "for name, optimal in avg_optimal.items():\n",
    "    # Application d'une moyenne mobile pour lisser la courbe\n",
    "    smoothed = np.convolve(optimal, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(np.arange(len(smoothed)), smoothed, label=name)\n",
    "plt.xlabel('Étapes')\n",
    "plt.ylabel('% d\\'actions optimales')\n",
    "plt.title('Pourcentage d\\'actions optimales au fil du temps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse du dilemme exploration/exploitation\n",
    "\n",
    "Nous allons maintenant analyser plus en détail le comportement des algorithmes en termes d'exploration et d'exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créons un nouvel environnement pour l'analyse détaillée\n",
    "np.random.seed(123)  # Pour la reproductibilité\n",
    "env_analysis = BanditEnvironment(k=10, distribution_type='bernoulli')\n",
    "env_analysis.visualize_arms()\n",
    "\n",
    "# Exécutons les algorithmes sur cet environnement\n",
    "algorithms_for_analysis = {\n",
    "    'ε-greedy (ε=0.1)': BanditAlgorithm(k=10, algorithm_type='epsilon-greedy', epsilon=0.1),\n",
    "    'UCB (c=2)': BanditAlgorithm(k=10, algorithm_type='ucb', c=2),\n",
    "    'Thompson Sampling': BanditAlgorithm(k=10, algorithm_type='thompson')\n",
    "}\n",
    "\n",
    "# Nombre de tirages par bras pour chaque algorithme\n",
    "n_pulls_per_arm = {name: np.zeros(10) for name in algorithms_for_analysis}\n",
    "\n",
    "# Simulation\n",
    "n_steps_analysis = 2000\n",
    "for name, algorithm in algorithms_for_analysis.items():\n",
    "    for _ in range(n_steps_analysis):\n",
    "        arm = algorithm.select_arm()\n",
    "        reward = env_analysis.pull(arm)\n",
    "        algorithm.update(arm, reward)\n",
    "    \n",
    "    # Enregistrement du nombre de tirages par bras\n",
    "    n_pulls_per_arm[name] = algorithm.n_pulls.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du nombre de tirages par bras\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, n_pulls) in enumerate(n_pulls_per_arm.items()):\n",
    "    axes[i].bar(range(10), n_pulls, alpha=0.7)\n",
    "    axes[i].axvline(x=env_analysis.optimal_arm, color='r', linestyle='--', \n",
    "                    label=f'Bras optimal')\n",
    "    axes[i].set_title(f'{name} - Distribution des tirages par bras')\n",
    "    axes[i].set_xlabel('Bras')\n",
    "    axes[i].set_ylabel('Nombre de tirages')\n",
    "    axes[i].set_xticks(range(10))\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. La valeur de l'information\n",
    "\n",
    "Une façon de comprendre le dilemme exploration/exploitation est de considérer la **valeur de l'information**. Lorsque nous explorons, nous sacrifions potentiellement une récompense immédiate pour obtenir de l'information qui pourrait mener à de meilleures récompenses futures.\n",
    "\n",
    "Développons une mesure simple pour évaluer la valeur de l'information pour chaque bras :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_value(q_values, n_pulls, total_steps, exploration_value=0.1):\n",
    "    \"\"\"Calcule une mesure simple de la valeur de l'information pour chaque bras\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    q_values : numpy.ndarray\n",
    "        Estimations des valeurs des bras\n",
    "    n_pulls : numpy.ndarray\n",
    "        Nombre de tirages par bras\n",
    "    total_steps : int\n",
    "        Nombre total d'étapes restantes\n",
    "    exploration_value : float\n",
    "        Paramètre de pondération pour l'exploration\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Valeurs d'information pour chaque bras\n",
    "    \"\"\"\n",
    "    # Estimation de l'incertitude (inverse du nombre de tirages)\n",
    "    uncertainty = 1.0 / np.sqrt(n_pulls + 1)\n",
    "    \n",
    "    # Valeur estimée de l'exploitation pure\n",
    "    exploitation_value = q_values\n",
    "    \n",
    "    # Valeur de l'information = valeur d'exploitation + terme d'exploration\n",
    "    info_value = exploitation_value + exploration_value * uncertainty * np.sqrt(np.log(total_steps))\n",
    "    \n",
    "    return info_value\n",
    "\n",
    "# Créons un nouvel environnement pour l'analyse\n",
    "np.random.seed(456)\n",
    "env_info = BanditEnvironment(k=5, distribution_type='bernoulli')\n",
    "env_info.visualize_arms()\n",
    "\n",
    "# Initialisons un algorithme ε-greedy\n",
    "algo_info = BanditAlgorithm(k=5, algorithm_type='epsilon-greedy', epsilon=0.1)\n",
    "\n",
    "# Simulation pour quelques étapes\n",
    "n_steps_info = 200\n",
    "info_values_history = []\n",
    "\n",
    "for t in range(n_steps_info):\n",
    "    # Calcul des valeurs d'information avant de prendre la décision\n",
    "    info_values = compute_information_value(\n",
    "        algo_info.q_values, \n",
    "        algo_info.n_pulls, \n",
    "        n_steps_info - t,\n",
    "        exploration_value=0.1\n",
    "    )\n",
    "    info_values_history.append(info_values.copy())\n",
    "    \n",
    "    # Sélection du bras et mise à jour\n",
    "    arm = algo_info.select_arm()\n",
    "    reward = env_info.pull(arm)\n",
    "    algo_info.update(arm, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'évolution des valeurs d'information\n",
    "info_values_history = np.array(info_values_history)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for arm in range(5):\n",
    "    plt.plot(info_values_history[:, arm], label=f'Bras {arm}')\n",
    "\n",
    "plt.axhline(y=env_info.optimal_mean, color='r', linestyle='--', \n",
    "           label=f'Moyenne optimale (μ={env_info.optimal_mean:.3f})')\n",
    "plt.xlabel('Étapes')\n",
    "plt.ylabel('Valeur d\\'information')\n",
    "plt.title('Évolution des valeurs d\\'information par bras')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
