{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Backpropagation with SGD from Scratch\n",
    "\n",
    "This notebook demonstrates the implementation of backpropagation with Stochastic Gradient Descent (SGD) using only NumPy.\n",
    "We solve the XOR problem using a simple feedforward neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Architecture\n",
    "\n",
    "We'll implement a simple feedforward neural network with configurable layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize a neural network with specified layer sizes.\n",
    "        \n",
    "        Parameters:\n",
    "        - layer_sizes: List of integers where each integer represents the number of neurons in a layer\n",
    "                      (e.g., [2, 3, 1] means 2 neurons in input layer, 3 in hidden layer, 1 in output layer)\n",
    "        - learning_rate: Learning rate for SGD\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        \n",
    "        # Initialize weights and biases with random values\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # Xavier initialization for weights\n",
    "        for i in range(1, self.num_layers):\n",
    "            # Initialize with small random values, scaled by sqrt(1/n)\n",
    "            scale = np.sqrt(1.0 / layer_sizes[i-1])\n",
    "            self.weights.append(np.random.randn(layer_sizes[i-1], layer_sizes[i]) * scale)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i])))\n",
    "        \n",
    "        # Lists to store values during forward pass (for backward pass)\n",
    "        self.z_values = []  # Weighted inputs\n",
    "        self.activations = []  # Outputs of each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid function.\n",
    "    \"\"\"\n",
    "    sig = sigmoid(z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function.\n",
    "    \"\"\"\n",
    "    return np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, X):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data (batch)\n",
    "    \n",
    "    Returns:\n",
    "    - Output of the network\n",
    "    \"\"\"\n",
    "    # Reset stored values\n",
    "    self.z_values = []\n",
    "    self.activations = [X]  # First activation is the input\n",
    "    \n",
    "    # Propagate through each layer\n",
    "    activation = X\n",
    "    for i in range(self.num_layers - 1):\n",
    "        # Calculate weighted input\n",
    "        z = np.dot(activation, self.weights[i]) + self.biases[i]\n",
    "        self.z_values.append(z)\n",
    "        \n",
    "        # Apply activation function\n",
    "        if i < self.num_layers - 2:  # Hidden layers use ReLU\n",
    "            activation = relu(z)\n",
    "        else:  # Output layer uses sigmoid\n",
    "            activation = sigmoid(z)\n",
    "            \n",
    "        self.activations.append(activation)\n",
    "    \n",
    "    return activation\n",
    "\n",
    "# Add the forward_propagation method to the NeuralNetwork class\n",
    "NeuralNetwork.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute binary cross-entropy loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True labels\n",
    "    - y_pred: Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    - Loss value\n",
    "    \"\"\"\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Add the compute_loss method to the NeuralNetwork class\n",
    "NeuralNetwork.compute_loss = compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self, X, y):\n",
    "    \"\"\"\n",
    "    Perform backpropagation to compute gradients.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data (batch)\n",
    "    - y: True labels\n",
    "    \n",
    "    Returns:\n",
    "    - Gradients for weights and biases\n",
    "    \"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    \n",
    "    # Initialize gradients\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = self.forward_propagation(X)\n",
    "    \n",
    "    # Backward pass\n",
    "    # Output layer error (delta)\n",
    "    delta = (y_pred - y) * sigmoid_derivative(self.z_values[-1])\n",
    "    \n",
    "    # Last layer gradients\n",
    "    nabla_w[-1] = np.dot(self.activations[-2].T, delta) / batch_size\n",
    "    nabla_b[-1] = np.sum(delta, axis=0, keepdims=True) / batch_size\n",
    "    \n",
    "    # Propagate error backwards through the network\n",
    "    for l in range(2, self.num_layers):\n",
    "        # Compute delta for current layer\n",
    "        z = self.z_values[-l]\n",
    "        delta = np.dot(delta, self.weights[-l+1].T) * relu_derivative(z)\n",
    "        \n",
    "        # Compute gradients\n",
    "        nabla_w[-l] = np.dot(self.activations[-l-1].T, delta) / batch_size\n",
    "        nabla_b[-l] = np.sum(delta, axis=0, keepdims=True) / batch_size\n",
    "    \n",
    "    return nabla_w, nabla_b\n",
    "\n",
    "# Add the backpropagation method to the NeuralNetwork class\n",
    "NeuralNetwork.backpropagation = backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(self, X_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Update weights and biases using SGD.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_batch: Input data batch\n",
    "    - y_batch: True labels batch\n",
    "    \"\"\"\n",
    "    # Compute gradients\n",
    "    nabla_w, nabla_b = self.backpropagation(X_batch, y_batch)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    for i in range(len(self.weights)):\n",
    "        self.weights[i] -= self.learning_rate * nabla_w[i]\n",
    "        self.biases[i] -= self.learning_rate * nabla_b[i]\n",
    "\n",
    "# Add the sgd_update method to the NeuralNetwork class\n",
    "NeuralNetwork.sgd_update = sgd_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the neural network using SGD.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Training data\n",
    "    - y: Training labels\n",
    "    - epochs: Number of training epochs\n",
    "    - batch_size: Size of mini-batches\n",
    "    - verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "    - History of losses\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data for each epoch\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.sgd_update(X_batch, y_batch)\n",
    "        \n",
    "        # Compute loss after each epoch\n",
    "        y_pred = self.forward_propagation(X)\n",
    "        loss = self.compute_loss(y, y_pred)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Add the train method to the NeuralNetwork class\n",
    "NeuralNetwork.train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predict Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    Make predictions on data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data\n",
    "    \n",
    "    Returns:\n",
    "    - Predictions\n",
    "    \"\"\"\n",
    "    return self.forward_propagation(X)\n",
    "\n",
    "# Add the predict method to the NeuralNetwork class\n",
    "NeuralNetwork.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example: XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the implemented neural network to solve the XOR problem.\n",
    "\n",
    "1. define points and labels for XOR problem\n",
    "2. train the neural network\n",
    "3. plot the loss curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizing Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the decision boundary of the trained neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
